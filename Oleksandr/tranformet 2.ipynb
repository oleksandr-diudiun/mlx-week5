{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (15.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow matplotlib sentencepiece pandas\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from both Parquet files\n",
    "train_0 = pd.read_parquet('../train/0000.parquet')\n",
    "train_1 = pd.read_parquet('../train/0001.parquet')\n",
    "train_2 = pd.read_parquet('../train/0002.parquet')\n",
    "train_3 = pd.read_parquet('../train/0003.parquet')\n",
    "\n",
    "# Concatenate the two DataFrames into one\n",
    "train = pd.concat([train_0, train_1, train_2, train_3], ignore_index=True)\n",
    "train['text'] = train['text'].str.slice(start=0, stop=2000)\n",
    "\n",
    "# Filter out elements with less than 2000 characters\n",
    "train = train[train['text'].str.len() >= 2000]\n",
    "# Reset the index of the filtered DataFrame and drop the old index\n",
    "# Select only the first 10000 rows\n",
    "train = train.iloc[:10000]\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "print(train.shape)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('spm_05_text_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, stories,  sp, input_tokens_amount, device):\n",
    "        super().__init__()\n",
    "        self.stories = stories\n",
    "        self.sp = sp\n",
    "        self.input_tokens_amount = input_tokens_amount\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_story = [sp.PieceToId('<s>')] + self.sp.EncodeAsIds(self.stories[idx])[:(self.input_tokens_amount-1)]\n",
    "        encoded_target = self.sp.EncodeAsIds(self.stories[idx])[:(self.input_tokens_amount-1)] + [sp.PieceToId('</s>')]\n",
    "        return torch.tensor(encoded_story, dtype=torch.long, device = self.device), torch.tensor(encoded_target, dtype=torch.long, device = self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_size, head_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.head_size      = head_size\n",
    "        self.embed_size     = embed_size\n",
    "        self.device         = device\n",
    "        \n",
    "        self.Key   = nn.Linear(self.embed_size, self.head_size, bias=False, device = self.device) # Size: [embed_size x head_size]\n",
    "        self.Query = nn.Linear(self.embed_size, self.head_size, bias=False, device = self.device) # Size: [embed_size x head_size] \n",
    "        self.Value = nn.Linear(self.embed_size, self.head_size, bias=False, device = self.device) # Size: [embed_size x head_size] \n",
    "        \n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batchSize, tokens, head_size = x.shape\n",
    "\n",
    "        key   = self.Key(x)    # Size: [batchSize x tokens x head_size]\n",
    "        query = self.Query(x)  # Size: [batchSize x tokens x head_size]\n",
    "        value = self.Value(x)  # Size: [batchSize x tokens x head_size]\n",
    "\n",
    "        tril = torch.tril(torch.ones(tokens, tokens, device = self.device))                 # Size: [tokens_amount x tokens_amount]. Diagonale ones left side only.                                                                      \n",
    "\n",
    "        # Compute Attention scores (\"Affinities\")\n",
    "        attention = query @ key.transpose(-2, -1) * head_size**0.5                          # [Batch Size x Tokens amount x head_size] @ [Batch Size x head_size x Tokens amount] --> [Batch Size x Tokens amount x Tokens amount]\n",
    "        attention = attention.masked_fill(tril[:tokens, :tokens] == 0, float('-inf'))       # Size: [batchSize x tokens x tokens]\n",
    "        attention = F.softmax(attention, dim=-1)                                            # Size: [batchSize x tokens x tokens]\n",
    "        attention = self.Dropout(attention)\n",
    "        \n",
    "        out = attention @ value                                                             # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        return out                                                                          # Size: [Batch Size x Tokens Amount x head_size]\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, head_size, dropout, device):\n",
    "        super().__init__()  \n",
    "        self.num_heads  = num_heads\n",
    "        self.head_size  = head_size\n",
    "        self.embed_size = embed_size \n",
    "        self.device     = device\n",
    "        \n",
    "        self.Heads = nn.ModuleList()\n",
    "        for _ in range(num_heads):\n",
    "            self.Heads.append(Head(self.embed_size, self.head_size, dropout, self.device)) # ModuleList Size: [num_heads]\n",
    "\n",
    "        self.Projection = nn.Linear(self.embed_size, self.embed_size)    # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        multiHead = torch.cat([head(x) for head in self.Heads], dim=-1)  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        projection = self.Dropout(self.Projection(multiHead))            # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        return projection                                                # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, dropout):\n",
    "        super().__init__()\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, attentions):\n",
    "        return self.FeedForward(attentions)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self,  embed_size, num_heads, head_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads  = num_heads\n",
    "        self.head_size  = head_size\n",
    "        self.device     = device\n",
    "\n",
    "        self.MultiAttentionHeads = MultiHeadAttention(self.embed_size, self.num_heads, self.head_size, dropout, self.device) # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.FeedForward         = FeedForward(self.embed_size, dropout)   # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.Ln1                 = nn.LayerNorm(self.embed_size)  # Size: [Batch Size x Tokens Amount x head_size]\n",
    "        self.Ln2                 = nn.LayerNorm(self.embed_size)  # Size: [Batch Size x Tokens Amount x head_size]\n",
    "\n",
    "    def forward(self, positionedEmbeddings):\n",
    "        attentions  = positionedEmbeddings + self.MultiAttentionHeads(self.Ln1(positionedEmbeddings)) # Size: [Batch Size x Tokens Amount x embed_size]. Apply MultiHead Attention Layer\n",
    "        feedForward = attentions + self.FeedForward(self.Ln2(attentions))                             # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        return feedForward                                                                  # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        \n",
    "\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, head_size, input_tokens_amount, vocab_size, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device                 = device\n",
    "        self.embed_size             = embed_size\n",
    "        self.input_tokens_amount    = input_tokens_amount\n",
    "        self.vocab_size             = vocab_size\n",
    "        self.num_heads              = num_heads\n",
    "        self.head_size              = head_size\n",
    "       \n",
    "        # self.register_buffer('positionEncodingBuffer', self.positionEncoding())  # Size: [1 (for batch dim) x input_tokens_amount x embed_size]\n",
    "        \n",
    "        self.Embedding = torch.nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embed_size, device = self.device) # Size: [vocab_size x embed_size]\n",
    "        self.Blocks    = nn.Sequential(\n",
    "            Block(self.embed_size, self.num_heads, self.head_size, dropout, self.device), # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            Block(self.embed_size, self.num_heads, self.head_size, dropout, self.device), # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            Block(self.embed_size, self.num_heads, self.head_size, dropout, self.device), # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            Block(self.embed_size, self.num_heads, self.head_size, dropout, self.device), # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            Block(self.embed_size, self.num_heads, self.head_size, dropout, self.device), # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            Block(self.embed_size, self.num_heads, self.head_size, dropout, self.device), # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "            nn.LayerNorm(self.embed_size),\n",
    "        )\n",
    "        self.LangModelHead  = nn.Linear(self.embed_size, self.vocab_size, device = self.device) # Size: [embed_size x vocab_size]\n",
    "\n",
    "    \n",
    "    def positionEncoding(self, input_tokens_amount):\n",
    "        positionEncoding = torch.zeros(input_tokens_amount, self.embed_size, device = self.device)                                  # max length x embedding dimmensions equivalent to Size: [input_tokens_amount x embed_size]\n",
    "        positions = torch.arange(0, input_tokens_amount, dtype=torch.float, device = self.device).unsqueeze(1)                      # Tensor [0, 1, 2,..., input_tokens_amount] -> [⋮] : rotated for each value in separate row of 1 column\n",
    "        div_term = torch.exp(torch.arange(0, self.embed_size, 2, device = self.device).float() * (-math.log(10000.0) / self.embed_size)) # Tensor [0, 2, 4,..., embed_size] x (-math.log(10000.0) / self.embed_size) --> exponenta\n",
    "\n",
    "        positionEncoding[:, 0::2] = torch.sin(positions * div_term)             # Size: [input_tokens_amount x embed_size], set the odd values (columns 1 and 3) \n",
    "        positionEncoding[:, 1::2] = torch.cos(positions * div_term)             # Size: [input_tokens_amount x embed_size], set the even values (columns 2 and 4) \n",
    " \n",
    "        return positionEncoding.unsqueeze(0)                                    # Size: [1 (for batch dim) x input_tokens_amount x embed_size]\n",
    "     \n",
    "    def forward(self, input):                                                   # Size: [Batch Size x Tokens Amount] - input\n",
    "        batchSize, tokens    = input.shape\n",
    "        embeddings           = self.Embedding(input)                            # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        positionedEmbeddings = embeddings + self.positionEncoding(tokens)       # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        blocks               = self.Blocks(positionedEmbeddings)                # Size: [Batch Size x Tokens Amount x embed_size]\n",
    "        logits               = self.LangModelHead(blocks)                       # Size: [Batch Size x Tokens Amount x vocab_size]\n",
    "        return logits                                                           # Size: [Batch Size x Tokens Amount x vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_size:  64\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batches = 32\n",
    "learning_rate = 1e-4\n",
    "dropout = 0.2\n",
    "maxNewTokens = 100\n",
    "input_tokens_amount = 128 # 3\n",
    "vocab_size = sp.GetPieceSize()\n",
    "embed_size = 512 # 512\n",
    "num_heads = 8 # 8\n",
    "head_size = int(embed_size / num_heads)\n",
    "print(\"head_size: \", head_size)\n",
    "if embed_size % num_heads != 0:\n",
    "    print(\"embed_size Cannot be divided evenly by num_heads.\")\n",
    "    sys.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modela and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(\n",
    "    embed_size          = embed_size, \n",
    "    num_heads           = num_heads,\n",
    "    head_size           = head_size, \n",
    "    input_tokens_amount = input_tokens_amount, \n",
    "    vocab_size          = vocab_size,\n",
    "    dropout             = dropout,\n",
    "    device              = device,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "sDataset = StoryDataset(train['text'], sp, input_tokens_amount, device)\n",
    "sDataloader = DataLoader(sDataset, batch_size=batches, shuffle=True, )\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (stories, targets) in enumerate(sDataloader): # stories Size: [Batch Size x Tokens Amount], targets Size: [Batch Size x Tokens Amount]\n",
    "        logits  = model(stories)                      # Size: [Batch Size x Tokens Amount x Vocab Size]\n",
    "        batch, tokens, vocabs = logits.shape\n",
    "        logits  = logits.view(batch * tokens, vocabs) # Size: [(Batch Size * Tokens Amount) x Vocab Size]\n",
    "        targets = targets.view(batch * tokens)        # Size: [(Batch Size * Tokens Amount)]\n",
    "        \n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:  # Print loss every 100 batches\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    startPhrase = \"One day, a little girl \"\n",
    "    startTokensIds = sp.EncodeAsIds(startPhrase)[:input_tokens_amount]     # [int, ..., tokens_length] \n",
    "    startTokensTensor = torch.tensor(startTokensIds, dtype=torch.long, device = device).unsqueeze(0)   # [1 x int, ..., tokens_length] \n",
    "    finalStoryTokensIds = startTokensIds\n",
    "\n",
    "    for _ in range(maxNewTokens):\n",
    "        lastTokensInSentence = startTokensTensor[:, -input_tokens_amount:]\n",
    "        genLogits = model(lastTokensInSentence)                         # Size: [Batch Size x Tokens Amount x Vocab Size]\n",
    "\n",
    "        # Let's focus only on last token in sequence\n",
    "        genLogits = genLogits[:, -1, :]                                 # Size: [Batch Size x Vocab Size]  \n",
    "        probabilities = F.softmax(genLogits, dim=-1)                    # Size: [Batch Size x Vocab Size], Probavilities of each word from vocab\n",
    "        nextIdx = torch.multinomial(probabilities, num_samples = 1)     # Size: [Batch Size x 1]\n",
    "\n",
    "        # apply running index to the running sequence \n",
    "        startTokensTensor = torch.cat((startTokensTensor, nextIdx), dim=1) # Size: [Batch Size x (Tokens Amount + 1)]\n",
    "        finalStoryTokensIds.append(nextIdx.item())\n",
    "        \n",
    "        finalStoryTokens = []\n",
    "        for tokenId in finalStoryTokensIds:\n",
    "            finalStoryTokens.append(sp.IdToPiece(tokenId))\n",
    "        \n",
    "    finalStory = ''.join(finalStoryTokens).replace('▁', ' ').strip()  # Assuming '▁' is the SentencePiece underline character\n",
    "    print(\"finalStory: \", finalStory)\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
