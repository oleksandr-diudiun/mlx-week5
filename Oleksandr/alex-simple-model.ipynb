{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyarrow  matplotlib sentencepiece pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train sentencepeace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allfiles = [\n",
    "#     '../train/0000.parquet',\n",
    "#     # '../train/0001.parquet',\n",
    "#     # '../train/0002.parquet',\n",
    "#     # '../train/0003.parquet',\n",
    "#     # '../validate/0000.parquet',\n",
    "#     ]\n",
    "\n",
    "# text_column = 'text'\n",
    "\n",
    "# # Initialize an empty list to store text\n",
    "# all_text = []\n",
    "\n",
    "# for file in allfiles:\n",
    "#     # Read the parquet file\n",
    "#     df = pd.read_parquet(file)\n",
    "#     # Append the text data to the list\n",
    "#     all_text.extend(df[text_column].tolist())\n",
    "\n",
    "# # Optional: Save all text to a single file if preferred\n",
    "# with open('0000_text.txt', 'w', encoding='utf-8') as f:\n",
    "#     for line in all_text:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SentencePiece model directly from Python list of sentences\n",
    "# spm.SentencePieceTrainer.train(input='all_text.txt', model_prefix='spm_full_text_model', vocab_size=10000, model_type='unigram')\n",
    "# spm.SentencePieceTrainer.train(input='0000_text.txt', model_prefix='spm_0000_text_model', vocab_size=10000, model_type='unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('../train/0000.parquet')\n",
    "# Select only the first 10000 rows\n",
    "train = train.iloc[:10000]\n",
    "\n",
    "validate = pd.read_parquet('../validate/0000.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, stories, sp_model_path):\n",
    "        self.stories = stories\n",
    "        # print(self.stories)\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(sp_model_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_story = [11000] + self.sp.EncodeAsIds(self.stories[idx])\n",
    "        encoded_target = self.sp.EncodeAsIds(self.stories[idx]) + [12000]\n",
    "        return torch.tensor(encoded_story, dtype=torch.long), torch.tensor(encoded_target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    stories, targets = zip(*batch)\n",
    "    \n",
    "    # Padding sequences to have the same length within a batch\n",
    "    padded_stories = pad_sequence(stories, batch_first=True, padding_value=13000)  # Assuming 0 is your padding ID\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=13000)  # Adjust padding_value if necessary\n",
    "    \n",
    "    return padded_stories, padded_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleTransformer(\n",
       "  (embed): Embedding(13001, 256)\n",
       "  (transformer_block): TransformerBlock(\n",
       "    (attention): SelfAttention(\n",
       "      (queries): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (keys): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (values): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=256, out_features=13001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Once upon a time Lilly rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy(logits, target, mask):\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_flat = target.view(-1)\n",
    "    mask_flat = mask.view(-1)\n",
    "    \n",
    "    losses = F.cross_entropy(logits_flat, target_flat, reduction='none')\n",
    "    masked_losses = losses * mask_flat\n",
    "    return masked_losses.sum() / mask_flat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 409])\n",
      "torch.Size([32, 409])\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8  # Based on your system's capabilities\n",
    "\n",
    "# Assuming `train_stories`, `train_targets` are your training data and targets\n",
    "train_dataset = StoryDataset(train['text'], \"spm_0000_text_model.model\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    "    # num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "for batch_idx, (stories, targets) in enumerate(train_loader):\n",
    "    print(stories.shape)\n",
    "    print(targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 9.628703117370605\n",
      "Epoch: 0, Batch: 10, Loss: 9.276326179504395\n",
      "Epoch: 0, Batch: 20, Loss: 8.86674690246582\n",
      "Epoch: 0, Batch: 30, Loss: 8.418627738952637\n",
      "Epoch: 0, Batch: 40, Loss: 7.980021953582764\n",
      "Epoch: 0, Batch: 50, Loss: 7.483315944671631\n",
      "Epoch: 0, Batch: 60, Loss: 7.119735240936279\n",
      "Epoch: 0, Batch: 70, Loss: 6.808167457580566\n",
      "Epoch: 0, Batch: 80, Loss: 6.437288284301758\n",
      "Epoch: 0, Batch: 90, Loss: 6.384188652038574\n",
      "Epoch: 0, Batch: 100, Loss: 6.164973258972168\n",
      "Epoch: 0, Batch: 110, Loss: 5.97415828704834\n",
      "Epoch: 0, Batch: 120, Loss: 5.946849346160889\n",
      "Epoch: 0, Batch: 130, Loss: 5.830953121185303\n",
      "Epoch: 0, Batch: 140, Loss: 5.768563270568848\n",
      "Epoch: 0, Batch: 150, Loss: 5.748751640319824\n",
      "Epoch: 0, Batch: 160, Loss: 5.648336410522461\n",
      "Epoch: 0, Batch: 170, Loss: 5.648838996887207\n",
      "Epoch: 0, Batch: 180, Loss: 5.565451622009277\n",
      "Epoch: 0, Batch: 190, Loss: 5.538675785064697\n",
      "Epoch: 0, Batch: 200, Loss: 5.475164890289307\n",
      "Epoch: 0, Batch: 210, Loss: 5.485043048858643\n",
      "Epoch: 0, Batch: 220, Loss: 5.351244926452637\n",
      "Epoch: 0, Batch: 230, Loss: 5.290709018707275\n",
      "Epoch: 0, Batch: 240, Loss: 5.262360095977783\n",
      "Epoch: 0, Batch: 250, Loss: 5.210743427276611\n",
      "Epoch: 0, Batch: 260, Loss: 5.316684246063232\n",
      "Epoch: 0, Batch: 270, Loss: 5.164804935455322\n",
      "Epoch: 0, Batch: 280, Loss: 5.1564741134643555\n",
      "Epoch: 0, Batch: 290, Loss: 5.277091026306152\n",
      "Epoch: 0, Batch: 300, Loss: 5.1980156898498535\n",
      "Epoch: 0, Batch: 310, Loss: 5.122853755950928\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have defined a model instance called `model`\n",
    "# Setting a custom starting learning rate for the Adam optimizer\n",
    "learning_rate = 1e-4  # Example: A smaller learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # Adjust based on your specific task\n",
    "\n",
    "EPOCHS = 1  # Number of epochs to train for\n",
    "\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (stories, targets) in enumerate(train_loader):\n",
    "        stories = stories.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        mask = (targets != 13000).float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(stories)  # Forward pass: compute the output\n",
    "        \n",
    "        # print(output.shape)\n",
    "        # print(targets.shape)\n",
    "\n",
    "        # Reshape outputs and targets to fit CrossEntropyLoss expectations\n",
    "        # Flatten the output and targets to pass into the custom loss function\n",
    "        # output_flat = output.view(-1, output.size(-1))  # Shape: [batch_size * seq_len, vocab_size]\n",
    "        # targets_flat = targets.view(-1)  # Shape: [batch_size * seq_len]\n",
    "        # mask_flat = mask.view(-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "        # Calculate the loss using the custom masked cross-entropy function\n",
    "        # loss = masked_cross_entropy(output_flat, targets_flat, mask_flat)\n",
    "        loss = masked_cross_entropy(output, targets, mask)\n",
    "\n",
    "        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "        # print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "        # break\n",
    "        if batch_idx % 10 == 0:  # Print loss every 100 batches\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "    break\n",
    "    # Validation step\n",
    "    val_loss = validate(model, validation_loader, criterion)\n",
    "    print(f'Epoch {epoch}, Training Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "    # Validation loop can go here\n",
    "    # Remember to set model.eval() and torch.no_grad() during validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_text = \"Once upon a time small\"\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"spm_0000_text_model.model\")\n",
    "encoded_input = sp.EncodeAsIds(initial_text)\n",
    "input_tensor = torch.tensor([encoded_input], dtype=torch.long).to(device)  # Assuming batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time small the park. He was a time, \"I. He was a time, \"I. He was a time, \"I. He was a time, \"I. She was a time, \"I. She was a time, \"I\n"
     ]
    }
   ],
   "source": [
    "max_length = 50  # Maximum length of the generated sequence\n",
    "eos_token_id = 12000  # Assuming '</s>' is your end-of-sequence token\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        output = model(input_tensor)  # Get the model's predictions\n",
    "        # Get the last predicted token (logits) as the next token. Shape of output: (batch_size, sequence_length, vocab_size)\n",
    "        last_token_logits = output[:, -1, :]\n",
    "        predicted_token_id = torch.argmax(last_token_logits, dim=-1).unsqueeze(0)  # Choose the token with the highest probability\n",
    "        # print(last_token_logits)\n",
    "        # print(predicted_token_id)\n",
    "        \n",
    "        # Append the predicted token ID to the input (which is fed into the model in the next iteration)\n",
    "        input_tensor = torch.cat((input_tensor, predicted_token_id), dim=1)\n",
    "        \n",
    "        # Check if the end-of-sequence token was generated\n",
    "        if predicted_token_id.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "# Decode the generated sequence back to text\n",
    "generated_sequence = [sp.IdToPiece(token_id) for token_id in input_tensor.squeeze().tolist()]\n",
    "generated_text = ''.join(generated_sequence).replace('▁', ' ').strip()  # Assuming '▁' is the SentencePiece underline character\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
